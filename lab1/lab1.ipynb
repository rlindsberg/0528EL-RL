{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x504 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "(0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-b08b5a57f3ba>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;31m# Create an environment maze\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m \u001B[0menv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmz\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMaze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmaze\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[0;31m# Finite horizon\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/rlindsberg/Git/28EL05-Reinforcement-Learning/lab1/maze.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, maze, weights, random_rewards)\u001B[0m\n\u001B[1;32m     48\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_actions\u001B[0m                \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mactions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_states\u001B[0m                 \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstates\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 50\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransition_probabilities\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__transitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     51\u001B[0m         self.rewards                  = self.__rewards(weights=weights,\n\u001B[1;32m     52\u001B[0m                                                 random_rewards=random_rewards)\n",
      "\u001B[0;32m~/Documents/rlindsberg/Git/28EL05-Reinforcement-Learning/lab1/maze.py\u001B[0m in \u001B[0;36m__transitions\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    106\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0ms\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_states\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    107\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0ma\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_actions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 108\u001B[0;31m                 \u001B[0mnext_s\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__move\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    109\u001B[0m                 \u001B[0mtransition_probabilities\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mnext_s\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    110\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mtransition_probabilities\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/rlindsberg/Git/28EL05-Reinforcement-Learning/lab1/maze.py\u001B[0m in \u001B[0;36m__move\u001B[0;34m(self, state, action)\u001B[0m\n\u001B[1;32m     91\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mstate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     92\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 93\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrow\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcol\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     94\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__transitions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: (0, 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import maze as mz\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "maze = np.zeros((7, 8))\n",
    "maze[0:4, 2] = 1 # svart ruta\n",
    "maze[5, 1:7] = 1\n",
    "maze[5, 1:7] = 1\n",
    "maze[6, 4] = 1\n",
    "maze[1:4, 5] = 1\n",
    "maze[2, 6:8] = 1\n",
    "# exit\n",
    "maze[6, 5] = 2\n",
    "mz.draw_maze(maze)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create an environment maze\n",
    "env = mz.Maze(maze)\n",
    "\n",
    "# Finite horizon\n",
    "horizon = 20\n",
    "# Solve the MDP problem with dynamic programming\n",
    "V, policy = mz.dynamic_programming(env, horizon)\n",
    "\n",
    "# Simulate the shortest path starting from position A\n",
    "method = 'DynProg'\n",
    "start  = (0, 0)\n",
    "path = env.simulate(start, policy, method)\n",
    "\n",
    "# Show the shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mz.animate_solution(maze, path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}